# Doc Scraper ğŸ“š

**é€šç”¨æ–‡æª”çˆ¬èŸ²å·¥å…·** - å°‡ä»»ä½•æŠ€è¡“æ–‡æª”ç¶²ç«™è½‰æ›ç‚º Markdown/CSV/Anki æ ¼å¼ï¼Œå»ºæ§‹ä½ çš„ã€ŒèªçŸ¥å¤–æ›ã€å­¸ç¿’ç³»çµ±ã€‚

---

## åŠŸèƒ½ç‰¹è‰²

- ğŸ¯ **é è¨­é…ç½®** - å…§å»ºå¤šç¨®å¸¸è¦‹æ–‡æª”æ¡†æ¶çš„é è¨­é…ç½®
- ğŸ”§ **é«˜åº¦å®¢è£½åŒ–** - æ”¯æ´è‡ªå®šç¾©é¸æ“‡å™¨å’Œåƒæ•¸
- ğŸ“„ **å¤šæ ¼å¼è¼¸å‡º** - Markdownã€CSVã€Anki å¡ç‰‡ã€Q&A å•ç­”å°
- ğŸ§  **NotebookLM æ•´åˆ** - å„ªåŒ–è¼¸å‡ºæ ¼å¼ï¼Œä¾¿æ–¼ RAG å­¸ç¿’
- ğŸƒ **Anki é–“éš”é‡è¤‡** - è‡ªå‹•ç”Ÿæˆé–ƒå¡ï¼Œå¼·åŒ–è¨˜æ†¶
- ğŸš€ **ç°¡å–®æ˜“ç”¨** - ä¸€è¡ŒæŒ‡ä»¤å³å¯é–‹å§‹çˆ¬å–
- ğŸ”„ **å¯é‡è¤‡ä½¿ç”¨** - å­¸ç¿’æ–°æŠ€è¡“æ™‚ç›´æ¥å¥—ç”¨

---

## å¿«é€Ÿé–‹å§‹

### å®‰è£

```bash
cd ~/Desktop/doc-scraper

# å®‰è£ä¾è³´
npm install

# å®‰è£ç€è¦½å™¨å¼•æ“ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ï¼‰
npx playwright install chromium

# å…¨åŸŸå®‰è£ï¼ˆå¯é¸ï¼Œè®“æŒ‡ä»¤æ›´æ–¹ä¾¿ï¼‰
npm link
```

### åŸºæœ¬ä½¿ç”¨

```bash
# æŸ¥çœ‹æ‰€æœ‰é è¨­é…ç½®
node scraper.js list

# çˆ¬å– Claude Code ä¸­æ–‡æ•™ç¨‹
node scraper.js scrape claude-code-cn

# çˆ¬å–ä»»æ„ç¶²ç«™ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
node scraper.js quick https://docs.example.com
```

---

## æŒ‡ä»¤èªªæ˜

### `list` - åˆ—å‡ºé è¨­é…ç½®

```bash
node scraper.js list
```

è¼¸å‡ºï¼š
```
ğŸ“š å¯ç”¨çš„é è¨­é…ç½®:

  claude-code-cn      Claude Code ä¸­æ–‡æ•™ç¨‹
  vitepress           VitePress æ–‡æª” (é€šç”¨)
  docusaurus          Docusaurus æ–‡æª” (é€šç”¨)
  gitbook             GitBook æ–‡æª” (é€šç”¨)
  readthedocs         Read the Docs (é€šç”¨)
  nextra              Nextra æ–‡æª” (é€šç”¨)
  custom              è‡ªå®šç¾©é…ç½®
```

### `scrape <preset>` - ä½¿ç”¨é è¨­é…ç½®çˆ¬å–

```bash
# åŸºæœ¬ç”¨æ³•
node scraper.js scrape <preset-name>

# å®Œæ•´åƒæ•¸
node scraper.js scrape <preset-name> [options]
```

**åƒæ•¸èªªæ˜ï¼š**

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `-u, --url <url>` | è¦†è“‹é è¨­ URL | é è¨­é…ç½®ä¸­çš„ URL |
| `-o, --output <dir>` | è¼¸å‡ºç›®éŒ„ | `./output` |
| `-c, --combined <file>` | åˆä½µæª”æ¡ˆåç¨± | `combined.md` |
| `-f, --format <format>` | è¼¸å‡ºæ ¼å¼: `md`, `csv`, `anki`, `all` | `md` |
| `--qa` | ç”Ÿæˆ Q&A å•ç­”å° | é—œé–‰ |
| `--summary` | ç”Ÿæˆç« ç¯€æ‘˜è¦ | é—œé–‰ |
| `--content-selector <s>` | è¦†è“‹å…§å®¹é¸æ“‡å™¨ | é è¨­é…ç½®ä¸­çš„é¸æ“‡å™¨ |
| `--link-selector <s>` | è¦†è“‹é€£çµé¸æ“‡å™¨ | é è¨­é…ç½®ä¸­çš„é¸æ“‡å™¨ |
| `--wait <ms>` | é é¢ç­‰å¾…æ™‚é–“ | 1500 |
| `--timeout <ms>` | é é¢è¶…æ™‚æ™‚é–“ | 30000 |
| `--max-pages <n>` | æœ€å¤§çˆ¬å–é æ•¸ | ç„¡é™åˆ¶ |
| `--no-headless` | é¡¯ç¤ºç€è¦½å™¨è¦–çª— | é—œé–‰ |

**ç¯„ä¾‹ï¼š**

```bash
# çˆ¬å– VitePress ç¶²ç«™
node scraper.js scrape vitepress --url https://vitepress.dev

# çˆ¬å–å‰ 10 é æ¸¬è©¦
node scraper.js scrape claude-code-cn --max-pages 10

# æŒ‡å®šè¼¸å‡ºç›®éŒ„
node scraper.js scrape docusaurus --url https://docusaurus.io --output ./my-docs
```

### `quick <url>` - å¿«é€Ÿçˆ¬å–

è‡ªå‹•åµæ¸¬ç¶²ç«™é¡å‹ï¼Œä½¿ç”¨é€šç”¨è¨­å®šçˆ¬å–ï¼š

```bash
node scraper.js quick https://any-docs-site.com
```

### `convert <input>` - æ ¼å¼è½‰æ›

å°‡å·²çˆ¬å–çš„ Markdown æª”æ¡ˆè½‰æ›ç‚ºå…¶ä»–æ ¼å¼ï¼š

```bash
# è½‰æ›ç‚º CSV
node scraper.js convert ./output/combined.md -f csv

# è½‰æ›ç‚º Anki å¡ç‰‡
node scraper.js convert ./output/combined.md -f anki -o my-cards.txt
```

---

## è¼¸å‡ºæ ¼å¼èªªæ˜

### Markdown (é è¨­)
- **æª”æ¡ˆ**: `combined.md` + `content/*.md`
- **ç”¨é€”**: NotebookLM åŒ¯å…¥ã€ä¸€èˆ¬é–±è®€
- **ç‰¹é»**: ä¿ç•™åŸå§‹çµæ§‹ï¼Œä¾¿æ–¼æœå°‹

### CSV
- **æª”æ¡ˆ**: `content.csv`
- **ç”¨é€”**: è©¦ç®—è¡¨åˆ†æã€è³‡æ–™è™•ç†
- **æ¬„ä½**: Index, Title, URL, Content_Length, Key_Points

### Anki å¡ç‰‡
- **æª”æ¡ˆ**: `anki-import.txt`
- **ç”¨é€”**: é–“éš”é‡è¤‡å­¸ç¿’
- **æ ¼å¼**: Tab åˆ†éš” (Question â†’ Answer â†’ Tags)
- **åŒ¯å…¥æ–¹å¼**: Anki â†’ File â†’ Import

### Q&A å•ç­”å°
- **æª”æ¡ˆ**: `qa-pairs.csv`
- **ç”¨é€”**: è‡ªæˆ‘æ¸¬é©—ã€AI è¨“ç·´è³‡æ–™
- **æ¬„ä½**: Chapter, Question, Answer, Context_URL

### JSON è³‡æ–™
- **æª”æ¡ˆ**: `data.json`
- **ç”¨é€”**: ç¨‹å¼åŒ–è™•ç†ã€é€²éšåˆ†æ
- **å…§å®¹**: å®Œæ•´çµæ§‹åŒ–è³‡æ–™

---

## è¼¸å‡ºæ ¼å¼ç¯„ä¾‹

```bash
# åƒ…è¼¸å‡º Markdownï¼ˆé è¨­ï¼‰
node scraper.js scrape claude-code-cn

# è¼¸å‡ºæ‰€æœ‰æ ¼å¼ + Q&A
node scraper.js scrape claude-code-cn -f all --qa

# åªè¦ Anki å¡ç‰‡
node scraper.js scrape vitepress --url https://vuejs.org -f anki
```

---

## å…§å»ºé è¨­é…ç½®

### claude-code-cn
- **åç¨±**: Claude Code ä¸­æ–‡æ•™ç¨‹
- **ç¶²å€**: https://claudecode.tangshuang.net
- **é©ç”¨**: å·²é…ç½®å®Œæˆï¼Œç›´æ¥ä½¿ç”¨

### vitepress
- **åç¨±**: VitePress æ–‡æª”
- **é©ç”¨**: VitePress æ¡†æ¶å»ºæ§‹çš„æ–‡æª”ç¶²ç«™
- **ç”¨æ³•**: `node scraper.js scrape vitepress --url <your-vitepress-site>`

### docusaurus
- **åç¨±**: Docusaurus æ–‡æª”
- **é©ç”¨**: Meta Docusaurus æ¡†æ¶å»ºæ§‹çš„æ–‡æª”ç¶²ç«™
- **ç”¨æ³•**: `node scraper.js scrape docusaurus --url <your-docusaurus-site>`

### gitbook
- **åç¨±**: GitBook æ–‡æª”
- **é©ç”¨**: GitBook å¹³å°æˆ– GitBook é¢¨æ ¼çš„æ–‡æª”
- **ç”¨æ³•**: `node scraper.js scrape gitbook --url <your-gitbook-site>`

### readthedocs
- **åç¨±**: Read the Docs
- **é©ç”¨**: Read the Docs å¹³å°è¨—ç®¡çš„æ–‡æª”ï¼ˆå¸¸è¦‹æ–¼ Python å°ˆæ¡ˆï¼‰
- **ç”¨æ³•**: `node scraper.js scrape readthedocs --url <your-rtd-site>`

### nextra
- **åç¨±**: Nextra æ–‡æª”
- **é©ç”¨**: Nextra (Next.js æ–‡æª”æ¡†æ¶) å»ºæ§‹çš„ç¶²ç«™
- **ç”¨æ³•**: `node scraper.js scrape nextra --url <your-nextra-site>`

---

## è‡ªå®šç¾©é…ç½®

### æ–¹æ³• 1ï¼šä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸

```bash
node scraper.js scrape custom \
  --url https://example.com/docs \
  --content-selector ".main-content" \
  --link-selector ".sidebar a"
```

### æ–¹æ³• 2ï¼šç·¨è¼¯ presets.json

åœ¨ `presets.json` ä¸­æ–°å¢è‡ªå®šç¾©é è¨­ï¼š

```json
{
  "my-custom-site": {
    "name": "æˆ‘çš„è‡ªå®šç¾©ç¶²ç«™",
    "url": "https://example.com",
    "indexPath": "/docs",
    "linkSelector": ".sidebar a",
    "linkFilter": "/docs/",
    "excludePatterns": ["#", "javascript:"],
    "contentSelector": ".markdown-body",
    "fallbackSelectors": ["article", "main"],
    "waitTime": 2000,
    "timeout": 30000
  }
}
```

**é…ç½®æ¬„ä½èªªæ˜ï¼š**

| æ¬„ä½ | èªªæ˜ |
|------|------|
| `name` | é¡¯ç¤ºåç¨± |
| `url` | ç¶²ç«™æ ¹ç¶²å€ |
| `indexPath` | æ–‡æª”ç›®éŒ„é è·¯å¾‘ |
| `linkSelector` | é€£çµçš„ CSS é¸æ“‡å™¨ |
| `linkFilter` | é€£çµ URL å¿…é ˆåŒ…å«çš„å­—ä¸² |
| `excludePatterns` | æ’é™¤çš„ URL æ¨¡å¼ |
| `contentSelector` | ä¸»è¦å…§å®¹çš„ CSS é¸æ“‡å™¨ |
| `fallbackSelectors` | å‚™ç”¨å…§å®¹é¸æ“‡å™¨ï¼ˆé™£åˆ—ï¼‰ |
| `waitTime` | é é¢æ¸²æŸ“ç­‰å¾…æ™‚é–“ (ms) |
| `timeout` | é é¢è¼‰å…¥è¶…æ™‚æ™‚é–“ (ms) |

---

## è¼¸å‡ºçµæ§‹

```
output/
â”œâ”€â”€ combined.md          # åˆä½µçš„å®Œæ•´ Markdownï¼ˆåŒ¯å…¥ NotebookLM ç”¨ï¼‰
â”œâ”€â”€ content.csv          # CSV æ ¼å¼ï¼ˆè©¦ç®—è¡¨åˆ†æï¼‰
â”œâ”€â”€ anki-import.txt      # Anki åŒ¯å…¥æª”ï¼ˆé–“éš”é‡è¤‡å­¸ç¿’ï¼‰
â”œâ”€â”€ qa-pairs.csv         # Q&A å•ç­”å°ï¼ˆè‡ªæˆ‘æ¸¬é©—ï¼‰
â”œâ”€â”€ data.json            # JSON çµæ§‹åŒ–è³‡æ–™ï¼ˆç¨‹å¼è™•ç†ï¼‰
â””â”€â”€ content/             # å€‹åˆ¥ç« ç¯€
    â”œâ”€â”€ 001-introduction.md
    â”œâ”€â”€ 002-getting-started.md
    â””â”€â”€ ...
```

---

## åŒ¯å…¥ NotebookLM

1. æ‰“é–‹ [NotebookLM](https://notebooklm.google.com)
2. å»ºç«‹æ–°ç­†è¨˜æœ¬
3. é»æ“Šã€ŒAdd sourceã€â†’ã€ŒUploadã€
4. ä¸Šå‚³ `combined.md` æª”æ¡ˆ
5. é–‹å§‹èˆ‡æ–‡æª”äº’å‹•å­¸ç¿’ï¼

**æ³¨æ„**ï¼š
- NotebookLM å–®ä¸€ä¾†æºé™åˆ¶ç´„ 500KB
- å¦‚æœæª”æ¡ˆéå¤§ï¼Œå¯åˆ†æ‰¹ä¸Šå‚³ `content/` è³‡æ–™å¤¾ä¸­çš„å€‹åˆ¥æª”æ¡ˆ

---

## åŒ¯å…¥ Anki

1. æ‰“é–‹ Anki æ¡Œé¢ç‰ˆ
2. é¸æ“‡ File â†’ Import
3. é¸å– `anki-import.txt` æª”æ¡ˆ
4. è¨­å®šï¼š
   - **Type**: Basic (and reversed card)
   - **Field separator**: Tab
   - **Allow HTML in fields**: å‹¾é¸
5. é»æ“Š Import

**å¡ç‰‡æ ¼å¼**ï¼š
- æ­£é¢ï¼šå•é¡Œï¼ˆå¦‚ã€Œä»€éº¼æ˜¯ Claude Codeï¼Ÿã€ï¼‰
- èƒŒé¢ï¼šé—œéµè¦é»æ‘˜è¦
- æ¨™ç±¤ï¼šç« ç¯€åç¨±

---

## å¸¸è¦‹å•é¡Œ

### Q: çˆ¬å–æ™‚å‡ºç¾ Timeout éŒ¯èª¤ï¼Ÿ

å¢åŠ è¶…æ™‚æ™‚é–“ï¼š
```bash
node scraper.js scrape <preset> --timeout 60000
```

### Q: å…§å®¹æŠ“å–ä¸å®Œæ•´ï¼Ÿ

å¢åŠ ç­‰å¾…æ™‚é–“è®“é é¢å®Œæ•´æ¸²æŸ“ï¼š
```bash
node scraper.js scrape <preset> --wait 3000
```

### Q: å¦‚ä½•åªçˆ¬å–éƒ¨åˆ†é é¢æ¸¬è©¦ï¼Ÿ

ä½¿ç”¨ `--max-pages` åƒæ•¸ï¼š
```bash
node scraper.js scrape <preset> --max-pages 5
```

### Q: æƒ³çœ‹åˆ°ç€è¦½å™¨æ“ä½œéç¨‹ï¼Ÿ

ä½¿ç”¨ `--no-headless` é¡¯ç¤ºç€è¦½å™¨ï¼š
```bash
node scraper.js scrape <preset> --no-headless
```

### Q: ç¶²ç«™æœ‰åçˆ¬èŸ²æ©Ÿåˆ¶ï¼Ÿ

1. å¢åŠ ç­‰å¾…æ™‚é–“ï¼š`--wait 5000`
2. æ¸›å°‘ä¸¦ç™¼ï¼šè…³æœ¬å·²å…§å»º 300ms é–“éš”
3. å¦‚ä»è¢«å°é–ï¼Œå¯èƒ½éœ€è¦æ‰‹å‹•è™•ç†

---

## å¯¦ç”¨ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šçˆ¬å– React å®˜æ–¹æ–‡æª”

```bash
node scraper.js scrape custom \
  --url https://react.dev \
  --content-selector "article" \
  --link-selector "nav a[href*='/reference'], nav a[href*='/learn']" \
  --output ./downloads/react-docs
```

### ç¯„ä¾‹ 2ï¼šçˆ¬å– Vue.js æ–‡æª”

```bash
node scraper.js scrape vitepress \
  --url https://vuejs.org \
  --output ./downloads/vue-docs
```

### ç¯„ä¾‹ 3ï¼šçˆ¬å– Python å¥—ä»¶æ–‡æª”

```bash
node scraper.js scrape readthedocs \
  --url https://requests.readthedocs.io \
  --output ./downloads/requests-docs
```

---

## ç›®éŒ„çµæ§‹

```
doc-scraper/
â”œâ”€â”€ scraper.js      # ä¸»ç¨‹å¼
â”œâ”€â”€ presets.json    # é è¨­é…ç½®æª”
â”œâ”€â”€ package.json    # å°ˆæ¡ˆé…ç½®
â”œâ”€â”€ README.md       # æœ¬èªªæ˜æ–‡ä»¶
â””â”€â”€ downloads/      # ä¸‹è¼‰è¼¸å‡ºç›®éŒ„ï¼ˆåŸ·è¡Œå¾Œç”¢ç”Ÿï¼‰
```

---

## ç¶­è­·è¨˜éŒ„

| æ—¥æœŸ | ç‰ˆæœ¬ | è®Šæ›´ |
|------|------|------|
| 2026-01-06 | 2.0.0 | æ–°å¢å¤šæ ¼å¼è¼¸å‡ºï¼šCSVã€Ankiã€Q&Aã€JSONï¼›æ–°å¢ convert æŒ‡ä»¤ |
| 2026-01-06 | 1.0.0 | åˆå§‹ç‰ˆæœ¬ |

---

## License

MIT License - åƒ…ä¾›å€‹äººå­¸ç¿’ä½¿ç”¨
